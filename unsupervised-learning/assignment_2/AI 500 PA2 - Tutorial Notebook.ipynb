{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1590726c",
   "metadata": {},
   "source": [
    "# Tutorial: PA2 – Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550bd8e",
   "metadata": {},
   "source": [
    "## Part 1: Image Segmentation with Clustering\n",
    "Task 1.1: Implementing K-means++ Initialization, K-means, and K-medoids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24376ba6",
   "metadata": {},
   "source": [
    "For kmeanspp_init function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53ad49",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.11.0)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"d:/02-personal/01_ms ai/ms/1st semester/courses/ai 500/assignments/assignment_2/venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Key steps breakdown:\n",
    "# 1. Initialize array to hold k centroids\n",
    "# 2. Pick first centroid randomly\n",
    "# 3. For remaining k-1 centroids:\n",
    "#    - Compute squared distances from each point to nearest centroid\n",
    "#    - Convert distances to probabilities (normalize)\n",
    "#    - Sample next centroid using these probabilities\n",
    "\n",
    "# Useful NumPy functions:\n",
    "# - np.random.randint(n) : random index\n",
    "# - np.linalg.norm(X - centroid, axis=1) : distances from all points to one centroid\n",
    "# - np.minimum(arr1, arr2) : element-wise minimum\n",
    "# - np.random.choice(n, p=probabilities) : weighted random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31232e46",
   "metadata": {},
   "source": [
    "K-means Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa18d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm structure:\n",
    "# 1. Initialize centroids (using init parameter)\n",
    "# 2. Repeat until convergence:\n",
    "#    a. Assignment step: assign each point to nearest centroid\n",
    "#    b. Update step: recalculate centroids as cluster means\n",
    "# 3. Check convergence (centroids don't change)\n",
    "\n",
    "# Key vectorization trick for assignment:\n",
    "# np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "# This creates distance matrix: shape (n_points, k_centroids)\n",
    "# Broadcasting: X[:, np.newaxis] creates shape (n, 1, d)\n",
    "#              centroids has shape (k, d)\n",
    "#              subtraction broadcasts to (n, k, d)\n",
    "#              norm along axis=2 gives (n, k)\n",
    "\n",
    "# Useful functions:\n",
    "# - np.argmin(distances, axis=1) : find nearest centroid for each point\n",
    "# - np.allclose(arr1, arr2) : check if arrays are approximately equal\n",
    "# - np.any(labels == i) : check if cluster i is non-empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8cd6c5",
   "metadata": {},
   "source": [
    "K-medoids Algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm structure:\n",
    "# 1. Initialize k medoids randomly from dataset\n",
    "# 2. Repeat until convergence:\n",
    "#    a. Assignment: assign points to nearest medoid\n",
    "#    b. Update: for each cluster, find point that minimizes total distance\n",
    "#       to all other points in cluster (new medoid)\n",
    "# 3. Check convergence (medoid indices don't change)\n",
    "\n",
    "# Finding new medoid for cluster i:\n",
    "# - Get all points in cluster i\n",
    "# - Compute pairwise distance matrix within cluster\n",
    "# - Sum distances for each point (to all others in cluster)\n",
    "# - Point with minimum sum becomes new medoid\n",
    "\n",
    "# Useful functions:\n",
    "# - np.where(labels == i)[0] : get indices of points in cluster i\n",
    "# - np.linalg.norm(points[:, np.newaxis] - points, axis=2) : pairwise distances\n",
    "# - np.sum(dist_matrix, axis=1) : sum distances for each point\n",
    "# - np.argmin(sums) : find point with minimum total distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac589729",
   "metadata": {},
   "source": [
    "Task 1.2: Calculating WCSS (Within-Cluster Sum of Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bccb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For calculate_wcss function:\n",
    "# 1. Initialize wcss = 0\n",
    "# 2. For each cluster i:\n",
    "#    a. Get all points assigned to cluster i\n",
    "#    b. Get centroid/medoid for cluster i\n",
    "#    c. Compute squared distances: (point - center)^2\n",
    "#    d. Sum these distances and add to wcss\n",
    "# 3. Return total wcss\n",
    "\n",
    "# Useful functions:\n",
    "# - np.unique(labels) : get number of clusters\n",
    "# - X[labels == i] : filter points in cluster i\n",
    "# - np.sum(np.square(differences), axis=1) : squared Euclidean distance\n",
    "# - Handle empty clusters: check len(cluster_points) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319f1fc",
   "metadata": {},
   "source": [
    "## Part 2: Anomaly Detection in Credit Card Transactions\n",
    "Task 2.1: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested analyses:\n",
    "# 1. Class distribution\n",
    "#    - Count fraud vs normal transactions\n",
    "#    - Visualize imbalance (bar plot)\n",
    "\n",
    "# 2. Feature distributions\n",
    "#    - Histograms for V1-V28 features\n",
    "#    - Compare fraud vs normal (overlaid distributions)\n",
    "\n",
    "# 3. Feature correlations\n",
    "#    - Correlation matrix heatmap\n",
    "#    - Identify highly correlated features\n",
    "\n",
    "# 4. Amount analysis\n",
    "#    - Box plots: fraud vs normal transaction amounts\n",
    "#    - Statistical summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13835c89",
   "metadata": {},
   "source": [
    "Task 2.2: Implementing DBSCAN\n",
    "-  For region_query function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6286b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Find all points within ε distance of a given point\n",
    "# Input: point_idx (index of query point)\n",
    "# Output: array of indices of neighbors\n",
    "\n",
    "# Implementation:\n",
    "# 1. Compute distances from query point to all points\n",
    "#    dists = np.linalg.norm(X - X[point_idx], axis=1)\n",
    "# 2. Return indices where distance <= eps\n",
    "#    return np.where(dists <= eps)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65580228",
   "metadata": {},
   "source": [
    "-  For expand_cluster function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96019584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Grow cluster by adding density-reachable points\n",
    "# Uses: nonlocal cluster_id (to modify outer scope variable)\n",
    "\n",
    "# Algorithm:\n",
    "# 1. Assign current point to cluster_id\n",
    "# 2. Iterate through neighbors (use while loop, not for):\n",
    "#    - If neighbor is noise (-1), assign to cluster\n",
    "#    - If neighbor unprocessed:\n",
    "#      - Query its neighbors\n",
    "#      - If it's a core point (enough neighbors):\n",
    "#        - Add its neighbors to expansion list\n",
    "# 3. Continue until no more points to add\n",
    "\n",
    "# Key insight: Use while loop because neighbors list grows during iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff76d3e",
   "metadata": {},
   "source": [
    "-  Main DBSCAN loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unvisited point:\n",
    "# 1. Find its neighbors\n",
    "# 2. If not enough neighbors: mark as noise (-1)\n",
    "# 3. Otherwise: expand_cluster and increment cluster_id\n",
    "\n",
    "# Note: Points initially marked as noise can later be \n",
    "# assigned to clusters if they're density-reachable from core points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1fa0f3",
   "metadata": {},
   "source": [
    "-  Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2D example with eps=1.5, min_samples=2\n",
    "# Points: A(0,0), B(1,0), C(5,5), D(6,5), E(10,10)\n",
    "# \n",
    "# Process A: neighbors={A,B} (2 points) → core point → cluster 0\n",
    "# Process B: already in cluster 0\n",
    "# Process C: neighbors={C,D} (2 points) → core point → cluster 1  \n",
    "# Process D: already in cluster 1\n",
    "# Process E: neighbors={E} (1 point) → noise (-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e654a7",
   "metadata": {},
   "source": [
    "## Part 3: Skin Detection via YCbCr + GMM\n",
    "Task 3.1: Implementing GMM with EM Algorithm\n",
    "-  For gaussian_pdf function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements multivariate Gaussian PDF formula\n",
    "# Inputs: X (n, d), mean (d,), cov (d, d)\n",
    "# Output: array of probabilities (n,)\n",
    "\n",
    "# Steps:\n",
    "# 1. Compute differences: diff = X - mean\n",
    "# 2. Compute inverse covariance: inv_cov = np.linalg.inv(cov)\n",
    "# 3. Compute exponent term: (diff @ inv_cov * diff).sum(axis=1)\n",
    "#    - This efficiently computes (x-μ)ᵀΣ⁻¹(x-μ) for all points\n",
    "# 4. Compute determinant: det = np.linalg.det(cov)\n",
    "# 5. Combine into formula\n",
    "\n",
    "# Useful trick: diff @ inv_cov * diff is efficient element-wise product\n",
    "# after matrix multiplication, equivalent to:\n",
    "# np.sum(diff @ inv_cov * diff, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b516893",
   "metadata": {},
   "source": [
    "-  For gmm_em function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "# - weights: uniform (1/k for each component)\n",
    "# - means: randomly sample k points from data\n",
    "# - covs: list of k covariance matrices (initialize with data covariance)\n",
    "\n",
    "# EM Loop:\n",
    "# E-step:\n",
    "#   1. Compute likelihoods: lik[i,k] = gaussian_pdf for component k\n",
    "#   2. Weight likelihoods: weighted_lik = lik * weights\n",
    "#   3. Compute responsibilities: resp = weighted_lik / sum(weighted_lik)\n",
    "#      - resp[i,k] = probability point i belongs to component k\n",
    "\n",
    "# M-step:\n",
    "#   1. Compute effective number of points: nk = sum(resp, axis=0)\n",
    "#   2. Update weights: weights = nk / n\n",
    "#   3. Update means: means[k] = weighted average of points\n",
    "#      means[k] = (resp[:,k] @ X) / nk[k]\n",
    "#   4. Update covariances:\n",
    "#      - Compute differences from mean\n",
    "#      - Weight by responsibilities\n",
    "#      - Add regularization: + np.eye(d) * 1e-6\n",
    "\n",
    "# Convergence:\n",
    "# - Compute log-likelihood: sum(log(weighted_lik.sum(axis=1)))\n",
    "# - Stop if change < tol or max_iter reached\n",
    "\n",
    "# Why regularization? Prevents singular covariance matrices\n",
    "# (matrices that can't be inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83029c71",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5092ae4",
   "metadata": {},
   "source": [
    "## SAMPLE APPROACHES TO ANALYTICAL QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96875027",
   "metadata": {},
   "source": [
    "These aren't trick questions—they're asking you to think about what you've observed, to experiment with parameters, and to explain why certain algorithms work better in certain contexts. The key here is to ground your answers in evidence: run experiments, create visualizations, reference your code outputs, or explain the underlying mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a80a14",
   "metadata": {},
   "source": [
    "**Note:** The following approaches are just ideas, you are **not required** to actually follow these samples exactly, you have liberty to design your own experiments as well. Also as mentioned in the main notebook, you can also refer back to the Core Tasks themselves if you feel they are sufficient to supplement your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892b971",
   "metadata": {},
   "source": [
    "-  In this tutorial, we shall cover just a couple of questions to give you an idea of what can be done for these parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754bd5ef",
   "metadata": {},
   "source": [
    "Question 1a: Effect of varying k in K-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-means++ with k = 3, 5, 7, 10\n",
    "# For each k:\n",
    "#   - Compute WCSS (decreases with k)\n",
    "#   - Compute MSE between original and segmented image\n",
    "#   - Visual inspection: count distinct regions\n",
    "\n",
    "def compute_mse(original, segmented):\n",
    "    # Compute mean squared error between original and segmented images\n",
    "    pass\n",
    "\n",
    "# Create comparison plot:\n",
    "# X-axis: k values\n",
    "# Y-axis: WCSS and MSE\n",
    "# Discuss: elbow point, diminishing returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caded4bf",
   "metadata": {},
   "source": [
    "Question 1d: Algorithm comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c267b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table from metrics\n",
    "# Analyze trade-offs:\n",
    "# - K-means++: fast, sensitive to initialization\n",
    "# - K-medoids: robust to outliers, slower\n",
    "# - Agglomerative: no need for k, hierarchical structure\n",
    "\n",
    "# Discuss why scores differ:\n",
    "# - Initialization quality\n",
    "# - Handling of outliers\n",
    "# - Cluster shape assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1271b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b30e4",
   "metadata": {},
   "source": [
    "Question 2a: Effect of k on fraud detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-means++ with k = 2, 3, 4, 5\n",
    "# For each k:\n",
    "#   - Identify anomaly cluster (smallest)\n",
    "#   - Compute precision, recall, F1\n",
    "#   - Plot metrics vs k\n",
    "\n",
    "# Expected observation:\n",
    "# - Higher k may split fraud cluster\n",
    "# - Recall dilution as fraud points distributed across clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5266957",
   "metadata": {},
   "source": [
    "Question 2b: DBSCAN hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429de888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each combination:\n",
    "#   - Run DBSCAN\n",
    "#   - Compute metrics\n",
    "#   - Create heatmap of recall vs (eps, min_samples)\n",
    "\n",
    "# Discuss trade-off:\n",
    "# - Tighter settings: fewer false positives, may miss fraud\n",
    "# - Looser settings: more noise detected as fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035f134",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07174a4",
   "metadata": {},
   "source": [
    "Question 3a: GMM components variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83051b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GMMs with k = 2, 3, 4, 5 components (just as an example)\n",
    "# For each k:\n",
    "#   - Compute likelihood ratio\n",
    "#   - Generate skin mask\n",
    "#   - Compute true/false positive rates (if ground truth available)\n",
    "#   - Count bounding boxes\n",
    "\n",
    "# Discuss:\n",
    "# - More components: better fit, but risk overfitting\n",
    "# - Optimal k balances flexibility and generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb4d97",
   "metadata": {},
   "source": [
    "Question 3b: EM convergence parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e210e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test combinations:\n",
    "tolerances = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "max_iters = [50, 100, 200]\n",
    "\n",
    "# For each combination:\n",
    "#   - Measure fitting time\n",
    "#   - Compute final log-likelihood\n",
    "#   - Evaluate mask quality (visual inspection or metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d307859",
   "metadata": {},
   "source": [
    "Question 3e: Likelihood ratio threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8df79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with thresholds: [0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "# For each:\n",
    "#   - Generate skin mask\n",
    "#   - Count and visualize bounding boxes\n",
    "#   - Compute average confidence per box\n",
    "\n",
    "# Create ROC-like analysis:\n",
    "# - Lower threshold: high sensitivity, low specificity\n",
    "# - Higher threshold: low sensitivity, high specificity\n",
    "\n",
    "# Discuss extraneous boxes:\n",
    "# - Background regions with similar color to skin\n",
    "# - Need balance based on application requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4729c42",
   "metadata": {},
   "source": [
    "The key here is to ground your answers in evidence: run experiments, create visualizations, reference your code outputs, or explain the underlying mathematics.\n",
    "\n",
    "You don't have to write essays—just conduct a small experiment, create a plot, or reference specific outputs from your code, and explain what you observe in 3-5 sentences. That's often enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c4a32",
   "metadata": {},
   "source": [
    "But make sure that the words you use for analysis are your own and not LLM generated. This is a **strict requirement**. Try to understand and address the problems yourself in order to gain a better apreciation for the concepts you learn in class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3e04c",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
