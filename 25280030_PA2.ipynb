{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rana-taqveem/ms-ai-ai-500/blob/main/25280030_PA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojo91lVgvPKh"
      },
      "source": [
        "# **AI 500 - PA2: Unsupervised Learning**\n",
        "\n",
        "**Name**:\n",
        "\n",
        "**Roll Number**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg5Yetx4vjJX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKpWKTCcvMo5"
      },
      "source": [
        "Before you start the assignment, here's an important clarification: this assignment is divided into two types of graded components—**Tasks** and **Questions**.\n",
        "\n",
        "-  <span style=\"color: purple; font-size: 20px;\">**Tasks**</span>: involve hands-on programming and require you to implement specific functionalities or solve problems using code.\n",
        "-  <span style=\"color: green; font-size: 20px;\">**Questions**</span>: are focused on theoretical and analytical interpretation, requiring you to analyze, explain, or draw insights from the tasks you’ve completed. You are **required** to either conduct additional analysis or to reference the coding <span style=\"color: purple; font-size: 20px;\">**Tasks**</span> to supplement your answers to these questions.\n",
        "\n",
        "\n",
        "Attempting both components is mandatory, as they are designed to complement each other.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- For the cells marked as #DO NOT CHANGE THIS CELL, you are still required to run those cells in order for your PA to be marked as complete. Please be careful about this.\n",
        "\n",
        "-  When working in Jupyter notebooks, avoid reusing the same variable names across different cells. This can cause overwriting and errors. To avoid this, try using more descriptive variable names to keep track of their purpose and prevent mix-ups.\n",
        "\n",
        "-  All cells in the notebook must be executed before submission and should display the expected results (graphs, plots, etc.). Any failure to run the cells and display the results will result in point deductions. Please ensure that the notebook is fully functional and the outputs are visible for review.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT8eZr8GPu47"
      },
      "source": [
        "### **MARK SCHEME**\n",
        "\n",
        "\n",
        "**Coding Tasks (Total 5 tasks, 44 marks)**\n",
        "\n",
        "- 3 tasks: 10 marks each (30 marks total) – For core implementations.\n",
        "- Remaining 2 tasks: 7 marks each (14 marks total) – For additional supportive tasks.\n",
        "\n",
        "**Analytical Questions (Total 14 questions, 56 marks)**\n",
        "\n",
        "- Main analysis per question: 2 marks (28 marks total) – For the core reasoning and insights.\n",
        "- Supporting reference (code/math/supplementary per question): 2 marks (28 marks total) – Consistent across all, for evidence-based backing (e.g., code snippets, equations etc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjalqEiAovNk"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqQB1fzXxdZ1"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "- **Naming Convention**: Name your submission file using the format `RollNumber_PA2.ipynb`.\n",
        "- **File Submission**: Submit only the `.ipynb` file (Jupyter Notebook) containing your complete code, markdown cells, and outputs. You are not required to submit any prompt logs, but please maintain those on your own end in case of any misunderstanding.\n",
        "- **Deadline**: Submit your assignment by **11:55 PM PKT on Wednesday, October 15, 2025**. Late submissions will incur a penalty as per the **Late Submission Policy** detailed in the outline.\n",
        "\n",
        "Good luck, and ensure all tasks are fully implemented and documented!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S29Pf8ajxdZ2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vdupVSrowqZ"
      },
      "source": [
        "## IMPORTS\n",
        "-  You are not allowed to change these\n",
        "-  You cannot import any additional libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CH9U_IgySxer"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # For numerical operations and array manipulations\n",
        "import matplotlib.pyplot as plt  # For plotting and visualization\n",
        "import pandas as pd # For data handling and CSV file reading\n",
        "from sklearn.preprocessing import StandardScaler # For feature scaling\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, silhouette_score  # For evaluation metrics\n",
        "from sklearn.decomposition import PCA # For dimensionality reduction (PCA for visualization)\n",
        "import requests # For downloading images or data from URLs (used in Tasks 1 and 2)\n",
        "from io import BytesIO # For handling binary data streams (used with requests)\n",
        "from PIL import Image # For image processing (used in Tasks 1 and 2)\n",
        "import cv2 # For computer vision tasks\n",
        "from sklearn.manifold import TSNE # Again useful for NL dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrjhBQWAvtnh"
      },
      "source": [
        "## **Part 1:** Image Segmentation with Clustering (- marks)\n",
        "\n",
        "**Objective:** This task explores the application of unsupervised clustering techniques to partition an image into meaningful regions based on pixel similarities. By implementing algorithms like K-means and K-medoids, you will investigate how to automatically segment visual data, which is crucial for tasks like object recognition and image editing, offering a foundation for understanding pattern recognition in computer vision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdoRmCxES11R"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "\n",
        "url = \"https://media.wired.com/photos/5b7c67dff521ce3ac9ba45e9/16:9/w_2240,h_1260,c_limit/post10%5Bhttps-_goo.gl_maps_g65Rg5BDBsQ2%5D-(1).jpg\"\n",
        "response = requests.get(url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "img = np.array(img)\n",
        "\n",
        "img_resized = np.array(Image.fromarray(img).resize((100, 100)))\n",
        "height, width, _ = img_resized.shape\n",
        "pixels = img_resized.reshape(-1, 3).astype(float) / 255.0  # normalize\n",
        "\n",
        "def rgb_to_ycbcr(rgb):\n",
        "    r, g, b = rgb[:, 0], rgb[:, 1], rgb[:, 2]\n",
        "    y = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "    cb = 0.5 * b - 0.1687 * r - 0.3313 * g + 0.5\n",
        "    cr = 0.5 * r - 0.4187 * g - 0.0813 * b + 0.5\n",
        "    return np.column_stack((y, cb, cr))\n",
        "\n",
        "pixels_ycbcr = rgb_to_ycbcr(pixels)\n",
        "\n",
        "features = pixels_ycbcr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_lXuFLyH3SO"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(img)\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QplgQCrCwgSN"
      },
      "source": [
        "<span style=\"color: purple; font-size: 20px;\">**Task 1.1:**</span>\n",
        "\n",
        "In this task, you will implement clustering algorithms to group data points based on their features. Your goal is to create functions for initializing and performing two popular clustering techniques. Follow these steps to complete the task:\n",
        "\n",
        "-  Implement a smart **initialization method for K-means clustering** that selects initial centroids to improve convergence.\n",
        "-  Develop the **K-means algorithm**, allowing for a custom initialization option to enhance clustering efficiency.\n",
        "-  Create a **K-medoids algorithm**, which uses actual data points as cluster representatives instead of means, focusing on robustness to outliers.\n",
        "-  Use NumPy for efficient vectorized operations to assign points to clusters and update centroids or medoids.\n",
        "\n",
        "Hint: Pay attention to the iteration logic and convergence checks in each algorithm. Start by understanding how distances are calculated and minimized to form clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e9Cz3N8_TlH_"
      },
      "outputs": [],
      "source": [
        "def kmeanspp_init(X, k):\n",
        "    n = X.shape[0]\n",
        "    centroids = np.zeros((k, X.shape[1]), dtype=np.float64)\n",
        "    all_distances = np.full((n, k), np.inf, dtype=np.float64)\n",
        "    min_distances = np.full(n, np.inf, dtype=np.float64)\n",
        "    probabilities = np.zeros(n, dtype=np.float64)\n",
        "    #YOUR CODE HERE\n",
        "\n",
        "    centroids[0] = X[np.random.randint(n)]\n",
        "    for counter in range(0, k-1):\n",
        "        for i, pixel in enumerate(X):\n",
        "            all_distances[i, counter] = np.sum(np.square(centroids[counter] - pixel))\n",
        "\n",
        "        min_distances = np.min(all_distances, axis=1)\n",
        "        total_distance = float(np.sum(min_distances))\n",
        "\n",
        "        if total_distance == 0:\n",
        "            probabilities = np.full(n, 1/n)\n",
        "        else:\n",
        "            for i, distance in enumerate(min_distances):\n",
        "                probabilities[i] = distance/total_distance\n",
        "\n",
        "        centroids[counter+1] = X[np.random.choice(len(min_distances), p=probabilities)]\n",
        "\n",
        "    return centroids\n",
        "\n",
        "def kmeanspp_init_opt(X, k):\n",
        "    n = X.shape[0]\n",
        "    centroids = np.zeros((k, X.shape[1]), dtype=np.float64)\n",
        "    all_distances = np.full((n, k), np.inf, dtype=np.float64)\n",
        "    min_distances = np.full(n, np.inf, dtype=np.float64)\n",
        "    probabilities = np.zeros(n, dtype=np.float64)\n",
        "\n",
        "    centroids[0] = X[np.random.randint(n)]\n",
        "    for counter in range(0, k-1):\n",
        "        X_L2_norm = np.sum(X**2, axis=1)\n",
        "        Ci_L2_norm = np.sum(centroids[counter]**2)\n",
        "        X_dot_Ci = 2 * np.dot(X, centroids[counter])\n",
        "\n",
        "        all_distances = X_L2_norm + Ci_L2_norm - X_dot_Ci\n",
        "\n",
        "        min_distances = np.minimum(min_distances, all_distances)\n",
        "        min_distances = np.clip(min_distances, 0, None)\n",
        "        total_distance = float(np.sum(min_distances))\n",
        "\n",
        "        if total_distance == 0:\n",
        "            probabilities = np.full(n, 1/n)\n",
        "        else:\n",
        "            probabilities = np.divide(min_distances, total_distance)\n",
        "\n",
        "        centroids[counter+1] = X[np.random.choice(len(min_distances), p=probabilities)]\n",
        "\n",
        "    return centroids\n",
        "\n",
        "def kmeans(X, k, max_iter=100, init='random'):\n",
        "    n, d = X.shape\n",
        "    if init == 'kmeanspp':\n",
        "        centroids = kmeanspp_init(X, k)\n",
        "    else:\n",
        "        centroids = X[np.random.choice(n, k, replace=False)]\n",
        "\n",
        "    #YOUR CODE HERE\n",
        "\n",
        "    iteration_num = 0\n",
        "    while True and iteration_num <= max_iter:\n",
        "\n",
        "        all_distances = np.full((n, k), np.inf, dtype=np.float64)\n",
        "        min_distances = np.full(n, np.inf, dtype=np.float64)\n",
        "\n",
        "        # using loops to compute distance\n",
        "        for i, centroid in enumerate(centroids):\n",
        "            for j, pixel in enumerate(X):\n",
        "                all_distances[j, i] = np.sqrt(np.sum(np.square(centroid - pixel)))\n",
        "\n",
        "        centroid_indices = np.argmin(all_distances, axis=1)\n",
        "\n",
        "        uniqure_centiords, inverse_indices, counts = np.unique(centroid_indices, return_counts=True, return_inverse=True)\n",
        "        avg_y_for_centriod = np.bincount(inverse_indices, weights=X[:, 0])/counts\n",
        "        avg_Cb_for_centriod = np.bincount(inverse_indices, weights=X[:, 1])/counts\n",
        "        avg_Cr_for_centriod = np.bincount(inverse_indices, weights=X[:, 2])/counts\n",
        "        new_centroids = np.zeros((k, X.shape[1]), dtype=float)\n",
        "\n",
        "        for i, new_centroid in enumerate(new_centroids):\n",
        "            new_centroid[0] = avg_y_for_centriod[i]\n",
        "            new_centroid[1] = avg_Cb_for_centriod[i]\n",
        "            new_centroid[2] = avg_Cr_for_centriod[i]\n",
        "\n",
        "        if np.allclose(centroids, new_centroids):\n",
        "            break  # Centroids have converged\n",
        "\n",
        "        centroids = new_centroids.copy()\n",
        "        iteration_num +=1\n",
        "    return inverse_indices, centroids\n",
        "\n",
        "def kmeans_optimized(X, k, max_iter=100, init='random'):\n",
        "    n, d = X.shape\n",
        "    if init == 'kmeanspp':\n",
        "        centroids = kmeanspp_init_opt(X, k)\n",
        "    else:\n",
        "        centroids = X[np.random.choice(n, k, replace=False)]\n",
        "\n",
        "    iteration_num = 0\n",
        "    while True and iteration_num <= max_iter:\n",
        "\n",
        "        # using the vector L2 Norm and Euclidean distance\n",
        "        # Features L2 Norm and reshaing to column vector to enable addition via broadcasting\n",
        "        #  X [[x1**2)],           Centriod [[ c1**2, c2**2, c3**2, c4**2, c5**2]]\n",
        "        #     [x2**2)]\n",
        "        #     [x3**2)]        +\n",
        "        #       .\n",
        "        #       .\n",
        "        #     [x10000**2)]]\n",
        "        #\n",
        "        X_L2 = np.sum(X**2, axis=1).reshape(-1, 1)                 # Features L2 Norm and reshaing to column vector to enable addition via broadcasting\n",
        "        Centroid_L2 = np.sum(centroids**2, axis=1).reshape(1, -1)  # Centroid Vector L2 Norm and reshaing to row vector to enable addition via broadcasting\n",
        "        X_dot_C = np.multiply(np.matmul(X, centroids.T), 2)        # computing dot product between features and centroids\n",
        "\n",
        "        # ||X-X||**2  = ||X|| + ||C|| - 2 X.Ct\n",
        "        all_distances = np.sqrt(X_L2 + Centroid_L2 - X_dot_C)\n",
        "\n",
        "        centroid_indices = np.argmin(all_distances, axis=1)\n",
        "\n",
        "        uniqure_centroids, inverse_indices, counts = np.unique(centroid_indices, return_counts=True, return_inverse=True)\n",
        "        new_centroids = np.zeros((k, X.shape[1]), dtype=float)\n",
        "        for f in range(X.shape[1]):\n",
        "            new_centroids[:, f] = np.bincount(inverse_indices, weights=X[:, f])/counts\n",
        "\n",
        "        if np.allclose(centroids, new_centroids):\n",
        "            break  # Centroids have converged\n",
        "\n",
        "        centroids = new_centroids.copy()\n",
        "        iteration_num +=1\n",
        "\n",
        "    return inverse_indices, centroids\n",
        "\n",
        "def kmedoids(X, k, max_iter=100):\n",
        "    n, d = X.shape\n",
        "    medoids_idx = np.random.choice(n, k, replace=False)\n",
        "    medoids = X[medoids_idx]\n",
        "\n",
        "    #YOUR CODE HERE\n",
        "\n",
        "    expanded = X[:, np.newaxis ]\n",
        "    pairwise_diff = expanded - X\n",
        "    pairwise_distances = np.linalg.norm(pairwise_diff, axis =2)\n",
        "\n",
        "    iteration_num = 0\n",
        "    while True and iteration_num <= max_iter:\n",
        "\n",
        "        all_distances = np.full((n, k), np.inf, dtype=np.float64)\n",
        "        new_mediods_idx = np.zeros(k, np.int64)\n",
        "\n",
        "        X_L2 = np.sum(X**2, axis=1).reshape(-1,1)\n",
        "        mediods_L2 = np.sum(medoids**2, axis=1).reshape(1,-1)\n",
        "        X_dot_C = np.multiply(np.matmul(X, medoids.T), 2)\n",
        "        all_distances = X_L2 + mediods_L2 - X_dot_C\n",
        "\n",
        "        centroid_indices = np.argmin(all_distances, axis=1)\n",
        "\n",
        "        unique_mediods, inverse_mediod_indices, counts = np.unique(centroid_indices, return_counts=True, return_inverse=True)\n",
        "        original_indices = np.arange(n)\n",
        "        sorted_mediod_indices =  original_indices[np.argsort(inverse_mediod_indices)]\n",
        "        sorted_inverse_mediod_indices = inverse_mediod_indices[np.argsort(inverse_mediod_indices)]\n",
        "        split_mediod_indices = np.nonzero(np.diff(sorted_inverse_mediod_indices))[0] + 1\n",
        "        mediod_wise_clusters = np.split(sorted_mediod_indices, split_mediod_indices)\n",
        "\n",
        "\n",
        "        for i, cluster in enumerate(mediod_wise_clusters):\n",
        "            cluster_distances = pairwise_distances[cluster][:, cluster]\n",
        "            sum_distances = np.sum( cluster_distances , axis =1)\n",
        "            best_medoid_idx = np.argmin(sum_distances)\n",
        "            new_mediods_idx[i] = cluster[best_medoid_idx]\n",
        "\n",
        "        if np.array_equal(medoids_idx, new_mediods_idx):\n",
        "            break  # Centroids have converged\n",
        "\n",
        "        medoids_idx = new_mediods_idx.copy()\n",
        "        medoids = X[medoids_idx]\n",
        "\n",
        "        # medoids = new_mediods.copy()\n",
        "        iteration_num +=1\n",
        "\n",
        "    return inverse_mediod_indices, medoids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC0bfqXSDL43"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "#We have defined the implementation of the Agglomerative Clustering algorithm for you.\n",
        "\n",
        "def agglomerative(X, k):\n",
        "    n = X.shape[0]\n",
        "    # since n is large, lets sample a subset\n",
        "    if n > 1000:\n",
        "        sample_idx = np.random.choice(n, 1000, replace=False)\n",
        "        X_sample = X[sample_idx]\n",
        "    else:\n",
        "        X_sample = X\n",
        "        sample_idx = np.arange(n)\n",
        "\n",
        "    m = X_sample.shape[0]\n",
        "    clusters = list(range(m))\n",
        "    dist_matrix = np.linalg.norm(X_sample[:, np.newaxis] - X_sample, axis=2)\n",
        "\n",
        "    while len(set(clusters)) > k:\n",
        "\n",
        "        # find closest pairs\n",
        "        min_dist = np.inf\n",
        "        pair = (-1, -1)\n",
        "        for i in range(m):\n",
        "            for j in range(i+1, m):\n",
        "                if clusters[i] != clusters[j] and dist_matrix[i, j] < min_dist:\n",
        "                    min_dist = dist_matrix[i, j]\n",
        "                    pair = (clusters[i], clusters[j])\n",
        "\n",
        "        # merge\n",
        "        for i in range(m):\n",
        "            if clusters[i] == pair[1]:\n",
        "                clusters[i] = pair[0]\n",
        "\n",
        "    # map to labels\n",
        "    unique_clusters = list(set(clusters))\n",
        "    label_map = {c: i for i, c in enumerate(unique_clusters)}\n",
        "    sample_labels = np.array([label_map[c] for c in clusters])\n",
        "\n",
        "    # assign all points to nearest sample cluster centers (approximate)\n",
        "    centers = np.array([X_sample[sample_labels == i].mean(axis=0) for i in range(k)])\n",
        "    distances = np.linalg.norm(X[:, np.newaxis] - centers, axis=2)\n",
        "    labels = np.argmin(distances, axis=1)\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LErEmTxxxdZ4"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL (except for the value of k, if you want to)\n",
        "val_k = 3\n",
        "X = features\n",
        "\n",
        "\n",
        "labels_kmeans, centroids_kmeans = kmeans_optimized(features, k=val_k, init='kmeanspp')\n",
        "labels_kmedoids, medoids = kmedoids(features, k=val_k)\n",
        "labels_agg = agglomerative(features, k=val_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDlHv-4oTvYA"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "def visualize_segmentation(img, labels, k, title):\n",
        "    segmented = np.zeros_like(img)\n",
        "    for i in range(k):\n",
        "        mask = (labels == i).reshape(img.shape[:2])\n",
        "        color = img[mask].mean(axis=0)\n",
        "        segmented[mask] = color.astype(int)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(segmented)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "visualize_segmentation(img_resized, labels_kmeans, val_k, 'K-means++ Segmentation')\n",
        "visualize_segmentation(img_resized, labels_kmedoids, val_k, 'K-medoids Segmentation')\n",
        "visualize_segmentation(img_resized, labels_agg, val_k, 'Agglomerative Segmentation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMW18mMMseVY"
      },
      "source": [
        "<span style=\"color: purple; font-size: 20px;\">**Task 1.2:**</span>\n",
        "\n",
        "In this task, you will implement a function to evaluate the quality of clustering by calculating the **Within-Cluster Sum of Squares (WCSS)**.  \n",
        "This metric measures the compactness of clusters. Follow these steps to complete the task:\n",
        "\n",
        "- Define a function that takes data points (`X`), cluster labels, and cluster centers (centroids or medoids) as inputs.  \n",
        "- Compute WCSS using the formula:\n",
        "\n",
        "$$\n",
        "\\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - c_i \\|^2\n",
        "$$\n",
        "\n",
        "where \\\\( k \\\\) is the number of clusters, \\\\( C_i \\\\) is the set of points in cluster \\\\( i \\\\), \\\\( x \\\\) is a point, and \\\\( c_i \\\\) is the cluster center.  \n",
        "\n",
        "- Use NumPy to calculate squared Euclidean distances between points and their assigned centers.  \n",
        "- Handle edge cases, such as empty clusters, by skipping them in the summation.  \n",
        "- Return the total WCSS score as a float.  \n",
        "\n",
        "**Hint:** Ensure the shape of inputs aligns correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bE568pJmsdcb"
      },
      "outputs": [],
      "source": [
        "def calculate_wcss(X, labels, centroids_or_medoids):\n",
        "    \"\"\"\n",
        "    Calculate Within-Cluster Sum of Squares manually.\n",
        "\n",
        "    Parameters:\n",
        "    - X: numpy array of shape (n_samples, n_features) containing the data points\n",
        "    - labels: numpy array of shape (n_samples,) containing cluster labels\n",
        "    - centroids_or_medoids: numpy array of shape (n_clusters, n_features) containing cluster centers\n",
        "\n",
        "    Returns:\n",
        "    - wcss: float, the total WCSS score\n",
        "    \"\"\"\n",
        "    unique_labels = np.unique(labels)\n",
        "    wcss = 0.0\n",
        "\n",
        "    # print(f\"labels: {labels}\")\n",
        "\n",
        "    # print(f\"Number of clusters: {n_clusters}\")\n",
        "    # print(f\"Centroids or Medoids shape: {centroids_or_medoids.shape}\")\n",
        "    # print(f\"Data points shape: {X.shape}\")\n",
        "    # print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "    for i, label in enumerate(unique_labels):\n",
        "      points_in_k = X[labels == label]\n",
        "      if points_in_k.shape[0] > 0:\n",
        "        wcss += np.sum((points_in_k - centroids_or_medoids[i])**2)\n",
        "\n",
        "    print(f\"wcss: {wcss}\")\n",
        "    return wcss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6ynycaOMFGy"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "\n",
        "def calculate_metrics(X, labels, centroids_or_medoids=None, algorithm_name=\"\"):\n",
        "    if len(set(labels)) < 2:\n",
        "        sil_score = 0\n",
        "    else:\n",
        "        sil_score = silhouette_score(X, labels)\n",
        "\n",
        "    if centroids_or_medoids is not None and len(np.unique(labels)) > 0:\n",
        "        wcss = calculate_wcss(X, labels, centroids_or_medoids)\n",
        "    else:\n",
        "        wcss = 0.0\n",
        "\n",
        "    print(f\"{algorithm_name} Metrics:\")\n",
        "    print(f\"  Silhouette Score: {sil_score:.4f}\")\n",
        "    print(f\"  Within-Cluster Sum of Squares (WCSS): {wcss:.4f}\\n\")\n",
        "    return sil_score, wcss\n",
        "\n",
        "sil_kmeans, wcss_kmeans = calculate_metrics(features, labels_kmeans, centroids_kmeans, \"K-means++\")\n",
        "sil_kmedoids, wcss_kmedoids = calculate_metrics(features, labels_kmedoids, medoids, \"K-medoids\")\n",
        "sil_agg, wcss_agg = calculate_metrics(features, labels_agg, np.array([features[labels_agg == i].mean(axis=0) for i in range(val_k) if np.any(labels_agg == i)]), \"Agglomerative\")\n",
        "\n",
        "algorithms = ['K-means++', 'K-medoids', 'Agglomerative']\n",
        "sil_scores = [sil_kmeans, sil_kmedoids, sil_agg]\n",
        "wcss_values = [wcss_kmeans, wcss_kmedoids, wcss_agg]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(algorithms, sil_scores, color=['blue', 'orange', 'green'])\n",
        "plt.title('Silhouette Scores by Algorithm')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(algorithms, wcss_values, color=['blue', 'orange', 'green'])\n",
        "plt.title('Within-Cluster Sum of Squares (WCSS) by Algorithm')\n",
        "plt.ylabel('WCSS')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "features_2d = pca.fit_transform(features)\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels_kmeans, cmap='viridis', alpha=0.5, s=10)\n",
        "plt.scatter(pca.transform(centroids_kmeans)[:, 0], pca.transform(centroids_kmeans)[:, 1], c='red', marker='x', s=200, label='Centroids')\n",
        "plt.title('K-means++ Clusters with Centroids')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels_kmedoids, cmap='viridis', alpha=0.5, s=10)\n",
        "plt.scatter(pca.transform(medoids)[:, 0], pca.transform(medoids)[:, 1], c='red', marker='x', s=200, label='Medoids')\n",
        "plt.title('K-medoids Clusters with Medoids')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels_agg, cmap='viridis', alpha=0.5, s=10)\n",
        "centroids_agg = np.array([features[labels_agg == i].mean(axis=0) for i in range(val_k) if np.any(labels_agg == i)])\n",
        "plt.scatter(pca.transform(centroids_agg)[:, 0], pca.transform(centroids_agg)[:, 1], c='red', marker='x', s=200, label='Centers')\n",
        "plt.title('Agglomerative Clusters with Centers')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z688-WJry4Sz"
      },
      "source": [
        "---\n",
        "## Analytical Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5CqzWX9y_sr"
      },
      "outputs": [],
      "source": [
        "# USE this space and add more cells below this to supplement the answers that you give to the questions that follow\n",
        "# This is RECOMMENDED. If you feel a question does not require additional analysis, you may refer to the coding tasks above\n",
        "# Grounding your responses in empirical evidence is standard practice in AI/ML research, so it is important that you support your results with evidence from either:\n",
        "\n",
        "#       - the coding tasks above\n",
        "#       - novel analysis conducted below\n",
        "#       - if questions are of a more foundational nature, then reference the underlying mathematics to back your response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOhKDke9z6tc"
      },
      "outputs": [],
      "source": [
        "# Your code here (optional)\n",
        "#---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfKNqiRmxMeM"
      },
      "source": [
        "<span style=\"color: green; font-size: 20px;\">**Question 1a:**</span> How does varying the number of clusters (k) in K-means++ (e.g., from 3 to 10) affect the segmentation quality, as measured by metrics like mean squared error (MSE) between the original and segmented images? Experiment with at least three different k values and discuss any observed over-segmentation or under-segmentation.\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkCEmb_hxdqS"
      },
      "source": [
        "<span style=\"color: green; font-size: 20px;\">**Question 1b:**</span> Based on  visualizations that you produce, critique the choice of YCbCr over RGB features—does it enhance color-based segmentation accuracy (e.g., better region boundaries)\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXwjnihdxp7e"
      },
      "source": [
        "<span style=\"color: green; font-size: 20px;\">**Question 1c:**</span> Explain how the Euclidean distance formula in K-means (minimizing the sum of squared distances from points to centroids) is implemented in the NumPy-based distance calculation step (np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)), and why this vectorized approach improves efficiency over a loop-based method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSYQJ28MxxFU"
      },
      "source": [
        "<span style=\"color: green; font-size: 20px;\">**Question 1d:**</span> Based on the computed Silhouette Scores and WCSS values for K-means++, K-medoids, and Agglomerative Clustering, compare how the algorithms perform in terms of cluster separation and compactness—why might one algorithm show higher or lower scores than the others, considering their underlying mechanisms like centroid initialization, medoid selection, or hierarchical merging?\n",
        "\n",
        "Answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxF1qTopx3v_"
      },
      "source": [
        "<span style=\"color: green; font-size: 20px;\">**Question 1e:**</span> Analyze the hierarchical nature of Agglomerative Clustering results: In what ways does it avoid the need for predefined k compared to centroid-based methods, and does this lead to more meaningful regions in the segmented image, or does it introduce biases from the bottom-up merging?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu7ZFGu22zxA"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JYJmkRmwobh"
      },
      "source": [
        "## **Part 2:** Anomaly Detection in Credit Card Transactions (- marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3cKy9Aew2o-"
      },
      "source": [
        "**Objective:** This task delves into detecting unusual patterns in financial data using clustering methods, focusing on identifying fraudulent credit card transactions. By exploring algorithms like K-means++, K-medoids, and DBSCAN, you will address a real-world challenge in cybersecurity and fraud prevention, highlighting the importance of anomaly detection in protecting economic systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB3tp-YVxdZ6"
      },
      "source": [
        "**Task Preparation:** Before starting the analysis, please download the required dataset from KAGGLE to work with the credit card fraud detection tasks. Follow these steps:\n",
        "\n",
        "- Visit the following link: [Credit Card Fraud Detection Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).\n",
        "- Download the `creditcard.csv` file directly, or extract from zipped folder after downloading the dataset.\n",
        "- Save the file in your working directory to ensure the code can access it, or add to runtime if working with Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3_kn3FhxdZ7"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE THIS CELL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c7TfzUV_VXUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeaf5e72-e89b-44a3-bb20-85086213ccd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled data: 5492 points, 492 fraud\n"
          ]
        }
      ],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# features: drop Time (not useful), use V1-V28 and Amount\n",
        "X = df.drop(['Time', 'Class'], axis=1).values\n",
        "y = df['Class'].values  # y is our target\n",
        "\n",
        "# preprocessing: standardize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# subsampling\n",
        "fraud_idx = np.where(y == 1)[0]\n",
        "normal_idx = np.random.choice(np.where(y == 0)[0], 5000, replace=False)\n",
        "sample_idx = np.concatenate((normal_idx, fraud_idx))\n",
        "X_sample = X_scaled[sample_idx]\n",
        "y_sample = y[sample_idx]\n",
        "\n",
        "print(f'Sampled data: {X_sample.shape[0]} points, {np.sum(y_sample)} fraud')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGQkxNOlqaul"
      },
      "source": [
        "<span style=\"color: purple; font-size: 20px;\">**Task 2.1:**</span> In this task, you are tasked with conducting an exploratory data analysis (EDA) of the Credit Card Fraud dataset. The objective is to gain initial insights into the data through provided analyses, which include summary statistics, distributions, and correlations. You are encouraged to extend this exploration by incorporating your own investigations, such as additional visualizations, feature correlation analyses, or anomaly pattern identification, to deepen your understanding of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzKyli9nqY-J"
      },
      "outputs": [],
      "source": [
        "#YOUR CODE HERE\n",
        "\n",
        "def generate_random_rgb_color():\n",
        "    \"\"\"Generates a random RGB color as a tuple (r, g, b).\"\"\"\n",
        "    return tuple(np.random.rand(3))\n",
        "\n",
        "# Example usage:\n",
        "random_color = generate_random_rgb_color()\n",
        "\n",
        "print(X.shape)\n",
        "transaction_classes = ['Normal', 'Fraud']\n",
        "\n",
        "# counts\n",
        "data = [X_sample.shape[0], np.sum(y_sample)]\n",
        "plt.bar(transaction_classes, data)\n",
        "plt.xlabel('Transaction Classes')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(nrows=7, ncols=4, figsize=(15,20))\n",
        "\n",
        "\n",
        "columns = ['V'+str(i) for i in range(1,29)]\n",
        "\n",
        "for column, ax in zip(columns, axes.flatten()):\n",
        "  data_normal = df[df['Class'] == 0][column].values\n",
        "  num_bins = 50\n",
        "  n, bins, _ = ax.hist(data_normal, num_bins, density=True, color='blue', alpha=0.5)\n",
        "  ax.set_label('Normal')\n",
        "  ax.set_title(f\"Distribution of Feature{column}\", fontweight='bold')\n",
        "\n",
        "  data_fraud = df[df['Class'] == 1][column].values\n",
        "  num_bins = 50\n",
        "  n, bins, _ = ax.hist(data_fraud, num_bins, density=True, color='red', alpha=0.5)\n",
        "  ax.set_label('Fraud')\n",
        "  ax.set_title(f\"Distribution of Feature{column}\", fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(columns)\n",
        "\n",
        "# Transaction Amount\n",
        "normal_ammounts = df[df['Class'] == 0]['Amount']\n",
        "fraud_ammounts = df[df['Class'] == 1]['Amount']\n",
        "data = [normal_ammounts, fraud_ammounts]\n",
        "plt.boxplot(data, showmeans=True)\n",
        "plt.ylim(0,5000)\n",
        "plt.xticks([1, 2], transaction_classes)\n",
        "plt.ylabel('Amount ($)')\n",
        "\n",
        "plt.title('Amount grouped by Normal vs Fraud Transactions')\n",
        "plt.show()\n",
        "\n",
        "# mean\n",
        "means = df.groupby('Class')['Amount'].mean()\n",
        "std = df.groupby('Class')['Amount'].std()\n",
        "plt.bar(transaction_classes, means.values, yerr=std.values, capsize=5)\n",
        "plt.ylabel('Mean Amount ($) and Std Spread')\n",
        "plt.xlabel('Transaction Classes')\n",
        "plt.title('Amount Mean and Std for Normal vs Fraud Transactions')\n",
        "plt.show()\n",
        "# group by class\n",
        "df.groupby('Class')['Amount'].describe()\n",
        "\n",
        "features_of_interest = ['V2', 'V3', 'V4', 'V10', 'V11', 'V12', 'V14', 'V16']\n",
        "\n",
        "fig1, axes1 = plt.subplots(nrows=2, ncols=4, figsize=(12,7))\n",
        "\n",
        "for foi, ax in zip(features_of_interest, axes1.flatten()):\n",
        "  normal = df[df['Class'] == 0][foi].dropna()\n",
        "  fraud = df[df['Class'] == 1][foi].dropna()\n",
        "  data = [normal, fraud]\n",
        "  ax.set_xticks([1, 2], transaction_classes)\n",
        "  ax.violinplot(data)\n",
        "  ax.set_label('Sensitivity Range')\n",
        "  ax.set_title(f\"{foi} for Normal vs Fraud \", fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "columns = correlation_matrix.columns\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "plt.imshow(correlation_matrix, cmap='autumn', interpolation='nearest')\n",
        "\n",
        "# Add colorbar\n",
        "plt.colorbar()\n",
        "\n",
        "plt.xticks(range(len(columns)), columns, rotation=90)\n",
        "plt.yticks(range(len(columns)), columns)\n",
        "plt.title(\"Heatmap with different color\")\n",
        "plt.show()\n",
        "\n",
        "#  df.groupby('Class')['V5'].describe()\n",
        "df.groupby('Class')['Amount'].describe()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH8eIDamFkgi"
      },
      "source": [
        "Add the relevant functions in the cell below, you may use your implementations from **Part 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WqG24ZtXxBXw"
      },
      "outputs": [],
      "source": [
        "# From Task 1 (copy the functions from the previous part)\n",
        "def kmeanspp_init_wrapper(X, k):\n",
        "    # n = X.shape[0]\n",
        "    # centroids = np.zeros((k, X.shape[1]))\n",
        "    # centroids[0] = X[np.random.randint(n)]\n",
        "    # distances = np.full(n, np.inf)\n",
        "\n",
        "    #YOUR CODE HERE\n",
        "\n",
        "    return kmeanspp_init(X, k)\n",
        "\n",
        "def kmeans_wrapper(X, k, max_iter=100, init='kmeanspp'):\n",
        "    # n, d = X.shape\n",
        "    # if init == 'kmeanspp':\n",
        "    #     centroids = kmeanspp_init(X, k)\n",
        "    # else:\n",
        "    #     centroids = X[np.random.choice(n, k, replace=False)]\n",
        "\n",
        "    # #YOUR CODE HERE\n",
        "\n",
        "    return kmeans_optimized(X, k, max_iter, init)\n",
        "\n",
        "def kmedoids_wrapper(X, k, max_iter=100):\n",
        "    # n, d = X.shape\n",
        "    # medoids_idx = np.random.choice(n, k, replace=False)\n",
        "    # medoids = X[medoids_idx]\n",
        "\n",
        "    # #YOUR CODE HERE\n",
        "\n",
        "    return kmedoids(X, k, max_iter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIlgv15L8kV1"
      },
      "source": [
        "<span style=\"color: purple; font-size: 20px;\">**Task 2.2:**</span> In this task, you will implement a density-based clustering algorithm to identify clusters and noise in a dataset. Your goal is to develop a function that groups points based on their density. Follow these steps to complete the task:\n",
        "\n",
        "- Implement the DBSCAN algorithm, which relies on the concept of density-reachable points within a specified radius (eps).  \n",
        "- Define a `region_query` function to calculate the Euclidean distance \\\\( \\| x_i - x_j \\| \\\\) between a point and all others, returning points within the eps radius.  \n",
        "- Create an `expand_cluster` function to grow clusters by checking if a point has at least `min_samples` neighbors, using the density-connectivity rule \\\\( \\text{number of points} \\geq \\min_samples \\\\).  \n",
        "- Assign labels where -1 indicates noise, and increment cluster IDs for core points with sufficient neighbors.  \n",
        "\n",
        "\n",
        "Hint: Focus on the iterative expansion process and how density thresholds determine cluster boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SuCJo2mxKuU"
      },
      "outputs": [],
      "source": [
        "def dbscan(X, eps, min_samples):\n",
        "    n = X.shape[0]\n",
        "    labels = np.full(n, -1)  # -1: noise\n",
        "    cluster_id = 0\n",
        "\n",
        "    print(f\"Shape of data: {X.shape}\")\n",
        "    # def region_query(point_idx):\n",
        "\n",
        "    #     #YOUR CODE HERE\n",
        "    #     pass\n",
        "\n",
        "    # def expand_cluster(point_idx, neighbors):\n",
        "    #     #YOUR CODE HERE\n",
        "    #     pass\n",
        "\n",
        "    # #YOUR CODE HERE\n",
        "\n",
        "    # return labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_jIKQxB22K7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f967f5b-9e1c-4e6b-c057-1bb12457d26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2265692234.py:119: RuntimeWarning: invalid value encountered in sqrt\n",
            "  all_distances = np.sqrt(X_L2 + Centroid_L2 - X_dot_C)\n"
          ]
        }
      ],
      "source": [
        "#DO NOT CHANGE THIS CELL (Unless you want to experiment with hyperparamters)\n",
        "\n",
        "\n",
        "# --------------K-means++ applied---------------\n",
        "labels_kmeans, _ = kmeans_wrapper(X_sample, k=3)\n",
        "\n",
        "cluster_sizes = np.bincount(labels_kmeans) # map anomaly cluster: we assume here that the smaller cluster is fraud (anomaly)\n",
        "anomaly_cluster_kmeans = np.argmin(cluster_sizes)\n",
        "pred_kmeans = (labels_kmeans == anomaly_cluster_kmeans).astype(int)\n",
        "\n",
        "# -----------K-medoids applied--------------\n",
        "labels_kmedoids, _ = kmedoids_wrapper(X_sample, k=3)\n",
        "cluster_sizes = np.bincount(labels_kmedoids)\n",
        "anomaly_cluster_kmedoids = np.argmin(cluster_sizes)\n",
        "pred_kmedoids = (labels_kmedoids == anomaly_cluster_kmedoids).astype(int)\n",
        "\n",
        "# -----------DBSCAN applied(tune eps and min_samples for the dataset)---------------\n",
        "\n",
        "# labels_dbscan = dbscan(X_sample, eps=4.0, min_samples=10)  # adjust based on data scale\n",
        "# noise (-1) as anomalies (fraud)\n",
        "# pred_dbscan = (labels_dbscan == -1).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ9ZMugexOws"
      },
      "source": [
        "Visualize Clusters (PCA + t-SNE for 2D Projection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrwzgnO_xOLp"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_sample)\n",
        "\n",
        "def plot_clusters_enhanced(X_pca, labels, y_true, title):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    if -1 in labels:\n",
        "        anomaly_mask = (labels == -1)\n",
        "    else:\n",
        "        anomaly_mask = (labels == np.argmin(np.bincount(labels[labels >= 0])))\n",
        "    plt.scatter(X_pca[~anomaly_mask, 0], X_pca[~anomaly_mask, 1], c='blue', label='Normal', alpha=0.5, s=10)\n",
        "    plt.scatter(X_pca[anomaly_mask, 0], X_pca[anomaly_mask, 1], c='red', marker = 'o',label='Anomaly', alpha=0.5, s=10)\n",
        "\n",
        "    plt.scatter(X_pca[y_true == 1, 0], X_pca[y_true == 1, 1], c='yellow',marker = 'x', label='True Fraud', s=50)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_clusters_enhanced(X_pca, labels_kmeans, y_sample, 'K-means++ Clusters with Anomalies')\n",
        "plot_clusters_enhanced(X_pca, labels_kmedoids, y_sample, 'K-medoids Clusters with Anomalies')\n",
        "plot_clusters_enhanced(X_pca, labels_dbscan, y_sample, 'DBSCAN Clusters with Anomalies')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keD6cBMv3gNq"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_tsne = tsne.fit_transform(X_sample)\n",
        "\n",
        "def plot_clusters_tsne(X_tsne, labels, y_true, title):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    if np.any(labels < 0):\n",
        "        anomaly_mask = (labels == -1)\n",
        "    else:\n",
        "        anomaly_mask = (labels == np.argmin(np.bincount(labels + (labels < 0).astype(int))))\n",
        "\n",
        "    plt.scatter(X_tsne[~anomaly_mask, 0], X_tsne[~anomaly_mask, 1], c='blue', label='Normal', alpha=0.5, s=10)\n",
        "\n",
        "    plt.scatter(X_tsne[anomaly_mask, 0], X_tsne[anomaly_mask, 1], c='red', marker = 'o', label='Anomaly', alpha=0.5, s=10)\n",
        "\n",
        "    plt.scatter(X_tsne[y_true == 1, 0], X_tsne[y_true == 1, 1], c='yellow', marker = 'x', label='True Fraud', s=50)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_clusters_tsne(X_tsne, labels_kmeans, y_sample, 'K-means++ Clusters with t-SNE')\n",
        "plot_clusters_tsne(X_tsne, labels_kmedoids, y_sample, 'K-medoids Clusters with t-SNE')\n",
        "plot_clusters_tsne(X_tsne, labels_dbscan, y_sample, 'DBSCAN Clusters with t-SNE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc8dBkT9xWwx"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "\n",
        "def evaluate(pred, y_true, algo_name):\n",
        "    sil = silhouette_score(X_sample, pred) if len(set(pred)) > 1 else 0\n",
        "    prec = precision_score(y_true, pred)\n",
        "    rec = recall_score(y_true, pred)\n",
        "    f1 = f1_score(y_true, pred)\n",
        "    print(f'{algo_name}:')\n",
        "    print(f'  Silhouette Score: {sil:.4f}')\n",
        "    print(f'  Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\\n')\n",
        "    return sil, prec, rec, f1\n",
        "\n",
        "metrics = {}\n",
        "metrics['K-means++'] = evaluate(pred_kmeans, y_sample, 'K-means++')\n",
        "metrics['K-medoids'] = evaluate(pred_kmedoids, y_sample, 'K-medoids')\n",
        "metrics['DBSCAN'] = evaluate(pred_dbscan, y_sample, 'DBSCAN')\n",
        "\n",
        "print('Note:')\n",
        "print('- Silhouette: Higher is better for clustering cohesion/separation.')\n",
        "print('- Precision/Recall/F1: Higher is better for anomaly (fraud) detection.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDBvMI9930Z8"
      },
      "source": [
        "---\n",
        "## Analytical Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-rCgOze32AZ"
      },
      "outputs": [],
      "source": [
        "# USE this space and add more cells below this to supplement the answers that you give to the questions that follow\n",
        "# This is RECOMMENDED. If you feel a question does not require additional analysis, you may refer to the coding tasks above\n",
        "# Grounding your responses in empirical evidence is standard practice in AI/ML research, so it is important that you support your results with evidence from either:\n",
        "\n",
        "#       - the coding tasks above\n",
        "#       - novel analysis conducted below\n",
        "#       - if questions are of a more foundational nature, then reference the underlying mathematics to back your response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo-WZD4S33YD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Your code here (optional)\n",
        "#---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rKIaoD94Cf3"
      },
      "source": [
        "<span style=\"color: green; font-size: 20px;\">**Question 2a:**</span> Examine how increasing the number of clusters (k from 2 to 5) in K-means++ influences fraud detection metrics (precision, recall, F1), particularly in identifying the anomaly cluster as the smallest one, and discuss any dilution of recall with higher k.\n",
        "\n",
        "Answer:\n",
        "\n",
        "<span style=\"color: green; font-size: 20px;\">**Question 2b:**</span> For DBSCAN, test variations in eps (e.g., 2.0 to 4.0) and min_samples (e.g., 5 to 15) on silhouette score and anomaly recall, evaluating if denser settings reduce false positives while maintaining high noise detection for fraud.\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "<span style=\"color: green; font-size: 20px;\">**Question 2c:**</span> Illustrate how DBSCAN's density-based expansion (core points with min_samples neighbors within eps) is implemented in the expand_cluster function using recursive neighbor queries, and contrast this with the mathematical definition of density-reachable points.\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "<span style=\"color: green; font-size: 20px;\">**Question 2d:**</span> Based on the comparative metrics, evaluate K-medoids' performance w.r.t your other algorithms? Any particular reason to justify its relative performance?\n",
        "\n",
        "Answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj-xNWkRHVkD"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7KUFj4nT2ie"
      },
      "source": [
        "## **Part 3:** Skin Detection via YCbCr + GMM (- marks)\n",
        "\n",
        "**Objective:** This task investigates skin detection in images using the Gaussian Mixture Model (GMM) in the YCbCr color space, exploring how to model complex skin tone distributions for applications like face detection. By implementing this from scratch, you will uncover the power of probabilistic modeling in image processing, essential for tasks in biometrics and human-computer interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inMgyzrvCWeS"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "\n",
        "\n",
        "url = 'https://st4.depositphotos.com/1022135/27218/i/450/depositphotos_272186204-stock-photo-group-young-people-hiking-mountain.jpg'\n",
        "\n",
        "response = requests.get(url)\n",
        "img_skin = Image.open(BytesIO(response.content))\n",
        "img_skin = np.array(img_skin)\n",
        "\n",
        "def rgb_to_ycbcr(rgb):\n",
        "    r, g, b = rgb[:, 0], rgb[:, 1], rgb[:, 2]\n",
        "    y = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "    cb = 0.5 * b - 0.1687 * r - 0.3313 * g + 0.5\n",
        "    cr = 0.5 * r - 0.4187 * g - 0.0813 * b + 0.5\n",
        "    return np.column_stack((y, cb, cr))\n",
        "\n",
        "pixels_skin = img_skin.reshape(-1, 3).astype(float) / 255.0\n",
        "ycbcr_skin = rgb_to_ycbcr(pixels_skin)\n",
        "\n",
        "# Get dimensions\n",
        "h, w = img_skin.shape[:2]\n",
        "\n",
        "skin_patches = [\n",
        "    (120, 170, 100, 160),   # (start_row, end_row, start_col, end_col)\n",
        "    (160, 220, 170, 220),\n",
        "    (160, 220, 270, 320),\n",
        "    (100, 150, 400, 460)\n",
        "]\n",
        "skin_indices = []\n",
        "for sr, er, sc, ec in skin_patches:\n",
        "    for row in range(sr, er):\n",
        "        for col in range(sc, ec):\n",
        "            skin_indices.append(row * w + col)\n",
        "skin_region = ycbcr_skin[skin_indices]\n",
        "\n",
        "\n",
        "non_skin_patches = [\n",
        "    (0, 50, 0, w),         # Top\n",
        "    (h-50, h, 0, w),       # Bottom\n",
        "    (150, 250, 20, 80),   # Backpack\n",
        "    (180, 280, 400, 500),    # Right trees\n",
        "    (300, 400, 200, 300)   # Ground\n",
        "]\n",
        "non_skin_indices = []\n",
        "for sr, er, sc, ec in non_skin_patches:\n",
        "    for row in range(sr, er):\n",
        "        for col in range(sc, ec):\n",
        "            non_skin_indices.append(row * w + col)\n",
        "non_skin_region = ycbcr_skin[non_skin_indices]\n",
        "\n",
        "# Use CbCr channels\n",
        "skin_cbcr = skin_region[:, 1:]\n",
        "non_skin_cbcr = non_skin_region[:, 1:]\n",
        "test_cbcr = ycbcr_skin[:, 1:]\n",
        "\n",
        "img_with_regions = cv2.cvtColor(img_skin, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "for i, (sr, er, sc, ec) in enumerate(skin_patches):\n",
        "    cv2.rectangle(img_with_regions, (sc, sr), (ec, er), (0, 255, 0), 2)  # green box\n",
        "    cv2.putText(img_with_regions, f'Skin {i+1}', (sc, sr - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "\n",
        "for i, (sr, er, sc, ec) in enumerate(non_skin_patches):\n",
        "    cv2.rectangle(img_with_regions, (sc, sr), (ec, er), (0, 0, 255), 2)  # red box\n",
        "    cv2.putText(img_with_regions, f'Non-Skin {i+1}', (sc, sr - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(cv2.cvtColor(img_with_regions, cv2.COLOR_BGR2RGB))\n",
        "plt.title('Viz Selected Regions')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEFUtr9YUL31"
      },
      "source": [
        "<span style=\"color: purple; font-size: 20px;\">**Task 3.1:**</span>  \n",
        "In this task, you will implement a Gaussian Mixture Model (GMM) using the Expectation-Maximization (EM) algorithm to model complex data distributions. Your goal is to develop functions that estimate the parameters of multiple Gaussian components. Follow these steps to complete the task:\n",
        "\n",
        "- Implement the `gaussian_pdf` function to compute the probability density function of a Gaussian distribution, using the formula:  \n",
        "  - \\\\( P(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right) \\\\)  \n",
        "  - Where \\\\( d \\\\) is the dimensionality, \\\\( \\mu \\\\) is the mean, \\\\( \\Sigma \\\\) is the covariance matrix, and use NumPy for matrix operations (e.g., inverse, determinant, and exponential).  \n",
        "\n",
        "- Create the `gmm_em` function to iteratively fit the GMM:  \n",
        "  - **E-step**: Calculate responsibilities (posterior probabilities) for each data point to belong to each Gaussian using the PDF and current parameters.  \n",
        "  - **M-step**: Update the weights, means, and covariances based on the responsibilities, ensuring numerical stability with regularization (e.g., small diagonal addition to covariances).  \n",
        "  - Use a log-likelihood convergence check to stop iterations, with a tolerance and maximum iteration limit.  \n",
        "\n",
        "- Initialize parameters randomly (e.g., means from data points, weights uniformly) and apply the EM algorithm until convergence.  \n",
        "\n",
        "\n",
        "**Hints**: Start by coding the Gaussian PDF with matrix operations for efficiency. For EM, implement the E-step by normalizing probabilities across components, and in the M-step, use weighted averages for updates. Add a small regularization term to avoid singular covariance matrices. Debug with a small dataset to verify convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph1olMGrT6xZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gaussian_pdf(X, mean, cov):\n",
        "    d = X.shape[1]\n",
        "\n",
        "    #  use identity matric of data point dimension and add a little jitter to avoid overfitting\n",
        "    regularization = np.eye( d ) * 1e-6\n",
        "    cov_reg = cov + regularization\n",
        "\n",
        "    #  computer inverse of regularized covariance matrix\n",
        "    cov_inverse = np.linalg.inv(cov_reg)\n",
        "\n",
        "    #  compute how far and in what direction data points lie by\n",
        "    # compute the difference between the data points and their mean\n",
        "\n",
        "    x_u = X- mean\n",
        "    temp = x_u @ cov_inverse\n",
        "    squared_term = temp * x_u\n",
        "    mahalanobis_squared = np.sum( squared_term , axis =1)\n",
        "    exp_result = np.exp(-0.5 * mahalanobis_squared )\n",
        "\n",
        "\n",
        "    #  calculate the normalization value to normalize the probability distribution\n",
        "    covariance_det = np.linalg.det(cov_reg)\n",
        "\n",
        "    inv_2_pi_d_half = np.power(2 * np.pi, -d / 2.0)\n",
        "    inv_det_half = np.power(covariance_det, -0.5)\n",
        "    normalization_c = inv_2_pi_d_half * inv_det_half\n",
        "\n",
        "    pdf = normalization_c * exp_result\n",
        "    return pdf\n",
        "\n",
        "\n",
        "def gmm_em(X, k, max_iter=50, tol=1e-4):\n",
        "    n, d = X.shape\n",
        "    weights = np.ones(k) / k\n",
        "    means = X[np.random.choice(n, k, replace=False)]\n",
        "    covs = [np.cov(X.T) + np.eye(d) * 1e-6 for _ in range(k)]\n",
        "\n",
        "    likelihoods = np.zeros((n, k))\n",
        "    responsibility_of_x = np.zeros((n, k))\n",
        "\n",
        "    iteration_num = 0\n",
        "    old_log_likelihood = -np.inf\n",
        "    log_lik_old = -np.inf\n",
        "    while True and iteration_num <= max_iter:\n",
        "        # E-step\n",
        "        for i in range(k):\n",
        "            pdf = gaussian_pdf(X, means[i], covs[i])\n",
        "            likelihoods[:, i] = weights[i] * pdf\n",
        "\n",
        "        # print(f\"Likelihoods shape: {likelihoods.shape}\")\n",
        "\n",
        "        weighted_likelihood_P = np.sum(likelihoods, axis=1)\n",
        "        # print(f\"Weighted Likelihood P shape: {weighted_likelihood_P.shape}\")\n",
        "        # print(f\"Weighted Likelihood P:  {weighted_likelihood_P}\")\n",
        "\n",
        "        responsibility_of_x = likelihoods / weighted_likelihood_P[:, np.newaxis]\n",
        "        # print(f\"Responsibility shape: {responsibility_of_x.shape}\")\n",
        "\n",
        "        log_likelihood = np.sum(np.log(weighted_likelihood_P))\n",
        "        # print(f\"Log Likelihood: {log_likelihood}\")\n",
        "\n",
        "        # M-step\n",
        "        # new weights\n",
        "        nk = np.sum(responsibility_of_x, axis=0)\n",
        "        # print(f\"N_k shape: {nk.shape}\")\n",
        "\n",
        "        new_weights = nk / n\n",
        "        # print(f\"New weights: {new_weights.shape}\")\n",
        "\n",
        "        # new means\n",
        "        # print(f\"Responsibility of x.T shape: {responsibility_of_x.T.shape}\")\n",
        "        # print(f\"X shape: {X.shape}\")\n",
        "\n",
        "        temp = (responsibility_of_x.T @ X)\n",
        "        # print(f\"Temp shape: {temp.shape}\")\n",
        "\n",
        "        # print(f\"N_k[:, np.newaxis] shape: {nk[:, np.newaxis].shape}\")\n",
        "        new_mean = temp / nk[:, np.newaxis]\n",
        "        # new_mean = temp / nk[:, np.newaxis]\n",
        "        # print(f\"New mean: {new_mean}\")\n",
        "\n",
        "        # new covariances\n",
        "        for i in range(k):\n",
        "            diff = X - new_mean[i].reshape(1, -1)\n",
        "            # print(f\"Diff shape {i}: {diff.shape}\")\n",
        "            # print(f\"Responsibility shape {i}: {responsibility_of_x[:, i].shape}\")\n",
        "            # print(f\"Responsibility of x[:, i] new shape {i}: {responsibility_of_x[:, i][:, np.newaxis].shape}\")\n",
        "\n",
        "            weighted_diff = responsibility_of_x[:, i][:, np.newaxis] * diff\n",
        "            # print(f\"Weighted diff shape {i}: {weighted_diff.shape}\")\n",
        "\n",
        "            weighted_diif_T = weighted_diff.T\n",
        "            # print(f\"Weighted diff T shape {i}: {weighted_diif_T.shape}\")\n",
        "            # print(f\"Diff shape {i}: {diff.shape}\")\n",
        "\n",
        "            product = weighted_diif_T @ diff\n",
        "            # print(f\"Product shape {i}: {product.shape}\")\n",
        "            # print(f\"N_k[i] shape {i}: {nk[i].shape}\")\n",
        "            new_cov = product / nk[i]\n",
        "\n",
        "            # print(f\"New cov shape {i}: {new_cov.shape}\")\n",
        "\n",
        "            regularization = np.eye( d ) * 1e-6\n",
        "            covs[i] = new_cov + regularization\n",
        "\n",
        "            # print(f\"Updated cov shape {i}: {covs[i].shape}\")\n",
        "\n",
        "\n",
        "        weights = new_weights\n",
        "        means = new_mean\n",
        "\n",
        "\n",
        "        if np.abs(log_likelihood - old_log_likelihood) < tol:\n",
        "            break\n",
        "\n",
        "        old_log_likelihood = log_likelihood\n",
        "        iteration_num +=1\n",
        "\n",
        "\n",
        "    return weights, means, covs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykMomArL9v-S"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL - (unless you want to play around with the components param)\n",
        "\n",
        "\n",
        "# Fit GMMs (use 3 components for flexibility)\n",
        "weights_skin, means_skin, covs_skin = gmm_em(skin_cbcr,3)\n",
        "weights_non, means_non, covs_non = gmm_em(non_skin_cbcr, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVaEQct3VQEw"
      },
      "source": [
        "Classify Pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gBvfYxkVSQ8"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL (you may experiment with hyperparams here if you want (e.g threshold))\n",
        "\n",
        "def gmm_likelihood(X, weights, means, covs):\n",
        "    k = len(weights)\n",
        "    lik = np.zeros(X.shape[0])\n",
        "    for i in range(k):\n",
        "        lik += weights[i] * gaussian_pdf(X, means[i], covs[i])\n",
        "    return lik\n",
        "\n",
        "lik_skin = gmm_likelihood(test_cbcr, weights_skin, means_skin, covs_skin)\n",
        "lik_non = gmm_likelihood(test_cbcr, weights_non, means_non, covs_non)\n",
        "ratio = lik_skin / (lik_non + 1e-10)\n",
        "threshold = 5 # Adjust as needed\n",
        "skin_mask = (ratio > threshold).reshape(img_skin.shape[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRBY-C2CUGLr"
      },
      "source": [
        "Visualize Skin Regions and Candidate Face Bounding Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7nnU9qPUD0k"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL\n",
        "\n",
        "plt.imshow(skin_mask, cmap='gray')\n",
        "plt.title('Skin Detection Mask')\n",
        "plt.show()\n",
        "\n",
        "contours, _ = cv2.findContours(skin_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "img_with_boxes = cv2.cvtColor(img_skin, cv2.COLOR_RGB2BGR)\n",
        "for cnt in contours:\n",
        "    if cv2.contourArea(cnt) > 250:\n",
        "        x, y, w, h = cv2.boundingRect(cnt)\n",
        "        cv2.rectangle(img_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))\n",
        "plt.title('Candidate Face Bounding Boxes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dthqk1WZZ851"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THIS CELL - Compute per-pixel probability of skin\n",
        "\n",
        "prob_skin = lik_skin / (lik_skin + lik_non + 1e-10)\n",
        "prob_skin = prob_skin.reshape(img_skin.shape[:2])\n",
        "\n",
        "img_with_boxes = cv2.cvtColor(img_skin, cv2.COLOR_RGB2BGR)\n",
        "for cnt in contours:\n",
        "    if cv2.contourArea(cnt) > 250:\n",
        "        x, y, w, h = cv2.boundingRect(cnt)\n",
        "\n",
        "        region_prob = prob_skin[y:y+h, x:x+w]\n",
        "        avg_prob = region_prob.mean()\n",
        "\n",
        "        cv2.rectangle(img_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "        cv2.putText(\n",
        "            img_with_boxes,\n",
        "            f\"{avg_prob*100:.1f}%\",\n",
        "            (x, y - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.6,\n",
        "            (0, 255, 0),\n",
        "            2\n",
        "        )\n",
        "\n",
        "plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))\n",
        "plt.title('Candidate Face Bounding Boxes with Probabilities')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrYzYAJf0lxN"
      },
      "source": [
        "---\n",
        "## Analytical Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zutJ7zsz0tBb"
      },
      "outputs": [],
      "source": [
        "# Use this space and add more cells below this to supplement the answers that you give to the questions that follow\n",
        "# This is RECOMMENDED. If you feel a question does not require additional analysis, you may refer to the coding tasks above\n",
        "# Grounding your responses in empirical evidence is standard practice in AI/ML research, so it is important that you support your results with evidence from either:\n",
        "\n",
        "#       - the coding tasks above\n",
        "#       - novel analysis conducted below\n",
        "#       - if questions are of a more foundational nature, then reference the underlying mathematics to back your response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmqGXBjs0z8N"
      },
      "outputs": [],
      "source": [
        "# Your code here (optional)\n",
        "#---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifjVw8KK0f4q"
      },
      "source": [
        "<span style=\"color: green; font-size: 20px;\">**Question 3a:**</span> Investigate how changing the number of Gaussian components (e.g., from 2 to 5) in the GMM for skin and non-skin models impacts the likelihood ratio classification accuracy, using the same threshold.\n",
        "\n",
        "Answer:\n",
        "\n",
        "<span style=\"color: green; font-size: 20px;\">**Question 3b:**</span> Explore the effects of varying the EM convergence tolerance (tol, e.g., from 1e-4 to 1e-6) and maximum iterations (e.g., 50 vs. 100) on GMM fitting time and skin mask quality, assessing if tighter tolerances reduce false positives in bounding box detection.\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "<span style=\"color: green; font-size: 20px;\">**Question 3c:**</span> Detail how the Gaussian probability density function (PDF) formula is coded in gaussian_pdf using matrix operations (diff @ inv_cov * diff), and explain why regularization (np.eye(d) * 1e-6) is added to the covariance matrix.\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "<span style=\"color: green; font-size: 20px;\">**Question 3d:**</span> Break down the Expectation-Maximization (EM) algorithm's E-step (responsibilities via likelihoods) and M-step (updating weights, means, covariances) as implemented in gmm_em, and how it mathematically models multi-modal distributions compared to single Gaussians.\n",
        "\n",
        "Answer:\n",
        "\n",
        "<span style=\"color: green; font-size: 20px;\">**Question 3e:**</span> Evaluate the likelihood ratio threshold's role in the skin mask: Why might a threshold of 1.0 result in extraneous bounding boxes (e.g., any small boxes that you might be seeing), and does adjusting it trade off sensitivity for specificity in face candidate detection?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y_EB1c-1s8n"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GPbDR997t-n"
      },
      "source": [
        "---**END OF PA2**---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}